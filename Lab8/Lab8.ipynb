{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF554 - Machine and Deep Learning\n",
    "## Lab 8 - Working with Text, Regularization and Recurrent Neural Networks\n",
    "\n",
    "In this lab we will be making some *sentiment analysis* with textual data from the IMDB dataset. In its simplest form, the *sentiment analysis* task consists in predicting the polarity of a text as positive or negative.\n",
    "\n",
    "First of all, let's take a look at the steps of text processing:\n",
    "\n",
    "1. Tokenization: break sentence into individual words\n",
    "    - Before: `\"PyTorch seems really easy to use!\"`\n",
    "    - After: `[\"PyTorch\", \"seems\", \"really\", \"easy\", \"to\", \"use\", \"!\"]`\n",
    "2. Building vocabulary: build an index of words associated with unique numbers\n",
    "    - Before: `[\"PyTorch\", \"seems\", \"really\", \"easy\", \"to\", \"use\", \"!\"]`\n",
    "    - After: `{\"Pytorch: 0, \"seems\": 1, \"really\": 2, ...}`\n",
    "3. Convert to numerals: map words to unique numbers (indices)\n",
    "    - Before: `{\"Pytorch: 0, \"seems\": 1, \"really\": 2, ...}`\n",
    "    - After: `[0, 1, 2, ...]`\n",
    "4. Embedding look-up: map sentences (indices now) to fixed matrices\n",
    "    - ```[[0.1, 0.4, 0.3],\n",
    "       [0.8, 0.1, 0.5],\n",
    "       ...]```\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's download the IMDB dataset (~85MB) and create the train, test and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "data['label']=data['sentiment'].replace(['positive', 'negative'], [1,0])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.5, random_state=1337)\n",
    "\n",
    "train, valid = train_test_split(train, test_size=0.3, random_state=1337)\n",
    "\n",
    "print(len(train), len(test), len(valid))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the *vocabulary* (step 2 above), mapping every word into a numerical id. We consider only the text in the training set for this step. We limit the vocabulary to the 1000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary\n",
    "words=[]\n",
    "num_words = 1000\n",
    "\n",
    "for text in train['review']:\n",
    "    tokens=tokenizer(text)\n",
    "    words.extend(tokens)\n",
    "\n",
    "top_1k = dict(Counter(words).most_common(1000))\n",
    "vocab = torchtext.vocab.vocab(top_1k, specials = ['<unk>', '<pad>'])\n",
    "#uncomment the following commented lines if you're using torchtext version 0.11\n",
    "#older versions do not support this way to build a vocabulary\n",
    "#vocab = torchtext.vocab.vocab(top_1k)\n",
    "#vocab.append_token('<unk>')\n",
    "#vocab.append_token('<pad>')\n",
    "vocab.set_default_index(vocab['<unk>']) #default index used when an unknown words is found\n",
    "\n",
    "print(train['review'].iloc[0])\n",
    "print(vocab.forward(tokenizer(train['review'].iloc[0]))) #example of how a sentence is transformed into a sequence of numerical IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now transform all reviews to vectors of word IDs. We also need to make all reviews of the same length. We set max_length to 80, therefore we'll cut reviews that exceed 80 words and pad those that are shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=80\n",
    "\n",
    "def vectorize_sentences(reviews, max_len):\n",
    "    vectors=[]\n",
    "    for text in reviews:\n",
    "        tokens=tokenizer(text)\n",
    "        v=vocab.forward(tokens)\n",
    "        if len(v) > max_len : v = v[:max_len]\n",
    "        if len(v) < max_len : #padding\n",
    "            tmp = np.full(max_len, vocab['<pad>'])\n",
    "            tmp[0:len(v)]=v \n",
    "            v = tmp\n",
    "        vectors.append(np.array(v))\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "train_X = vectorize_sentences(train['review'], max_len)\n",
    "test_X = vectorize_sentences(test['review'], max_len)\n",
    "val_X = vectorize_sentences(valid['review'], max_len)\n",
    "\n",
    "train_y = np.array(train['label']).reshape(-1,1)\n",
    "test_y = np.array(test['label']).reshape(-1,1)\n",
    "val_y = np.array(valid['label']).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create and load the batches for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# create tensor datasets\n",
    "trainset = TensorDataset(torch.from_numpy(train_X).to(device), torch.from_numpy(train_y).float().to(device))\n",
    "validset = TensorDataset(torch.from_numpy(val_X).to(device), torch.from_numpy(val_y).float().to(device))\n",
    "testset = TensorDataset(torch.from_numpy(test_X).to(device), torch.from_numpy(test_y).float().to(device))\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(validset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce now a fully connected NN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, max_len):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(max_len*embedding_dim, hidden_dim) \n",
    "        \n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.view(-1, max_len*embedding_dim)\n",
    "        # Linear function\n",
    "        out = self.fc1(embedded)\n",
    "\n",
    "        # Non-linear activation function\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        # Here we use a final sigmoid function as we are using BCE loss which does not implement it\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = num_words + 2 #add 2 for <unk> and <pad> symbols\n",
    "embedding_dim = 100\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, embedding_dim, hidden_dim, output_dim, max_len)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing problems are affected by the problem of features that occur infrequently. E.g., it is a lot less likely that we will see the word *phantasmagorical* than the word *reading*. Parameters associated with infrequent features only receive meaningful updates whenever these features occur. Given a decreasing learning rate we might end up in a situation where the parameters for common features converge rather quickly to their optimal values, whereas for infrequent features we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate either decreases too slowly for frequent features or too quickly for infrequent ones.\n",
    "\n",
    "A solution to this problem are *adaptive learning rates*. For instance, **Adagrad** is an optimization technique based on the accumulation of squared gradients.\n",
    "\n",
    "Here we use the notation $\\mathbf{g}_t = \\nabla_{\\mathbf{w}} l(y_t, f(\\mathbf{x}_t, \\mathbf{w}))$ for the gradient of the loss function at time $t$. We use the variable $\\mathbf{s}_t$ to accumulate past gradient variance.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{s}_t & = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2, \\\\\n",
    "    \\mathbf{w}_t & = \\mathbf{w}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\odot \\mathbf{g}_t.\n",
    "\\end{aligned}$$\n",
    "\n",
    "However Adagrad has some issues and for our case we are going to use an improved version called <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html\">**RMSprop**</a>. It uses a moving average of squared gradients to normalize the gradient. This normalization balances the step size (momentum), decreasing the step for large gradients to avoid exploding and increasing the step for small gradients to avoid vanishing:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{s}_t & \\leftarrow \\gamma \\mathbf{s}_{t-1} + (1 - \\gamma) \\mathbf{g}_t^2, \\\\\n",
    "    \\mathbf{w}_t & \\leftarrow \\mathbf{w}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\odot \\mathbf{g}_t.\n",
    "\\end{aligned}$$\n",
    "\n",
    "The constant $\\epsilon > 0$ is typically set to $10^{-6}$ to ensure that we do not suffer from division by zero or overly large step sizes. $\\gamma$ is usually set to 0.9.\n",
    "\n",
    "The following block sets up the network with <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\">Binary Cross Entropy</a> loss and RMSprop optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of groups of parameters\n",
    "print('Number of groups of parameters {}'.format(len(list(model.parameters()))))\n",
    "print('-'*50)\n",
    "# Print parameters\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function carries out the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optizmizer, loss_criterion):\n",
    "    iter = 0\n",
    "    num_epochs = 10\n",
    "    history_train_acc, history_val_acc, history_train_loss, history_val_loss = [], [], [], []\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (samples, labels) in enumerate(train_loader):\n",
    "            # Training mode\n",
    "            model.train()\n",
    "\n",
    "            # Load samples\n",
    "            samples = samples.view(-1, max_len).to(device)\n",
    "            labels = labels.view(-1, 1).to(device)\n",
    "\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(samples)\n",
    "\n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                # Get training statistics\n",
    "                train_loss = loss.data.item()\n",
    "\n",
    "                # Testing mode\n",
    "                model.eval()\n",
    "                # Calculate Accuracy         \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                # Iterate through test dataset\n",
    "                for samples, labels in valid_loader:\n",
    "                    # Load samples\n",
    "                    samples = samples.view(-1, max_len).to(device)\n",
    "                    labels = labels.view(-1).to(device)\n",
    "\n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(samples)\n",
    "\n",
    "                    # Val loss\n",
    "                    val_loss = criterion(outputs.view(-1, 1), labels.view(-1, 1))\n",
    "\n",
    "                    predicted = outputs.ge(0.5).view(-1)\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum().item()\n",
    "                    # correct = (predicted == labels.byte()).int().sum().item()\n",
    "\n",
    "                accuracy = 100. * correct / total\n",
    "\n",
    "                # Print Loss\n",
    "                print('Iter: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {}'.format(iter, train_loss, val_loss.item(), round(accuracy, 2)))\n",
    "\n",
    "                # Append to history\n",
    "                history_val_loss.append(val_loss.data.item())\n",
    "                history_val_acc.append(round(accuracy, 2))\n",
    "                history_train_loss.append(train_loss)\n",
    "\n",
    "                # Save model when accuracy beats best accuracy\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    # We can load this best model on the validation set later\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    return (history_train_acc, history_val_acc, history_train_loss, history_val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots the losses for the testing and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history_train_loss, history_val_loss):\n",
    "    # Set plotting style\n",
    "    #plt.style.use(('dark_background', 'bmh'))\n",
    "    plt.style.use('bmh')\n",
    "    plt.rc('axes', facecolor='none')\n",
    "    plt.rc('figure', figsize=(16, 4))\n",
    "\n",
    "    # Plotting loss graph\n",
    "    plt.plot(history_train_loss, label='Train')\n",
    "    plt.plot(history_val_loss, label='Validation')\n",
    "    plt.title('Loss Graph')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the behaviour of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_acc, val_acc, train_loss, val_loss) = train_model(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ques 1**: What can you deduce from this graph? Is the network working as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization techniques are required to reduce overfitting (i.e. the predictor fits too closely to the training data and does not generalize well to new data). Overfitting tends to occur when we have little data and a complex hypothesis class.\n",
    "\n",
    "A first idea for regularization is to introduce a *penalty term*, which makes it harder for the optimizer to return an overly flexible predictor.\n",
    "\n",
    "We will start by adding a regularization term to our (point-wise) loss:\n",
    "\n",
    "$$ loss_R = loss + \\frac{\\lambda}{2}\\left\\Vert W\\right\\Vert _{2}^{2}$$\n",
    "\n",
    "The gradient for the regularized loss is therefore:\n",
    "\n",
    "$$\\nabla_W loss_R = \\nabla_W loss + \\lambda W$$\n",
    "\n",
    "With pytorch, adding a regularization term is done on the optimization function by adding the parameter *weight_decay* (corresponding to the above $\\lambda$) to the call to the optimizer.\n",
    "\n",
    "> **Task 1**: set the optimizer *weight_decay* to 0.005 and re-train the model, studying its behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to take a look at the parameters of the model before and after regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_parameters(mdl):\n",
    "    weights = torch.Tensor().to(device)\n",
    "    for param_group in list(mdl.parameters()):\n",
    "        weights = torch.cat((param_group.view(-1), weights))\n",
    "    ws = weights.detach().cpu().numpy()\n",
    "    plt.hist(ws.reshape(-1), range=(-.5, .5), bins=501)\n",
    "\n",
    "look_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, embedding_dim, hidden_dim, output_dim, max_len)\n",
    "model.to(device)\n",
    "\n",
    "#enter here the code for Task 1\n",
    "optimizer = \n",
    "\n",
    "(train_acc, val_acc, train_loss, val_loss) = train_model(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_loss, val_loss)\n",
    "look_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Another solution for regularization in neural network consists in applying **Dropout**: at each step of training, a new subnetwork is selected. As a result an adaptation appears in the final network only if it exists in a sufficient part of the training data.\n",
    "\n",
    "<img src=\"figures/dropout.png\" alt=\"dropout\" width=\"400\"/><font size=\"1\">Image from (Srivastava et al. ,2014) Dropout: A Simple Way to Prevent Neural Networks from\n",
    "Overfitting, JMLR</font>\n",
    "\n",
    "We need to introduce a probability $p$ of deactivating a neuron in the hidden layers. So, if we have $n$ neurons in layer $l$, we expect to have $n * (1-p)$ active neurons.\n",
    "\n",
    "Therefore the output of a generic layer becomes:\n",
    "\n",
    "$$ z^l = \\alpha(W_l^\\top \\delta z^{l-1} + b_l) $$\n",
    "\n",
    "with $\\delta \\sim Bernoulli(p)$ and $\\alpha(x)$ an activation function.\n",
    "\n",
    "> **Task 2** : add <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\">dropout</a> to the neural network above, between the two fully connected layers, with p=0.8; re-train the new network and observe the result (note: use the optimizer without weight_decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModelWithDropout(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, max_len):\n",
    "        super(FeedforwardNeuralNetModelWithDropout, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(max_len*embedding_dim, hidden_dim) \n",
    "        \n",
    "        #insert here your code\n",
    "        \n",
    "        \n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        #print(\"embedding for word \", vocab.lookup_token(83))\n",
    "        #print(self.embedding(torch.Tensor([83]).int()))\n",
    "        \n",
    "        embedded = embedded.view(-1, max_len*embedding_dim)\n",
    "        # Linear function\n",
    "        out = self.fc1(embedded)\n",
    "\n",
    "        # Non-linearity\n",
    "        out = torch.relu(out)\n",
    "        \n",
    "        #insert here your code\n",
    "        \n",
    "        \n",
    "        # Take note here use a final sigmoid function so your loss should not go through sigmoid again as we are using BCE loss.\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "    \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here your code\n",
    "# Instantiate model class and assign to object\n",
    "model_dropout = \n",
    "\n",
    "model_dropout.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = \n",
    "\n",
    "(train_acc, val_acc, train_loss, val_loss) = train_model(model_dropout, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train your model here\n",
    "plot_losses(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM to classify textual data\n",
    "\n",
    "A fully connected neural network is not an ideal way of processing textual data. Recurrent Neural Networks are more useful for this task since they are more adequate to work on sequential data such as text.\n",
    "\n",
    "A particular type of RNN is the Long Short-Term Memory, or <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">LSTM</a>.\n",
    "\n",
    "> **Task 3** : complete the following code to implement a single layer, unidirectional LSTM with a fully connected layer taking the final hidden states of the LSTM as input. Note that you have to set the parameter *batch_first*=True for the LSTM layer as our dataset comes in this format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        \"\"\"\n",
    "        vocab_size: (int) size of the vocabulary - required by embeddings\n",
    "        embed_dim: (int) size of embeddings\n",
    "        hidden_dim: (int) number of hidden units\n",
    "        num_class: (int) number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        #enter here your code\n",
    "        \n",
    "       \n",
    "\n",
    "    def forward(self, text):\n",
    "        r\"\"\"\n",
    "        Arguments:\n",
    "            text: 1-D tensor representing a bag of text tensors\n",
    "        \"\"\"\n",
    "        #ENTER HERE YOUR CODE\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 4**: run the LSTM model on the IMDB dataset and verify whether it is overfitting or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your code here\n",
    "\n",
    "lstm = \n",
    "lstm.to(device)\n",
    "\n",
    "optimizer = \n",
    "\n",
    "(train_acc, val_acc, train_loss, val_loss) = train_model(lstm, optimizer, criterion)\n",
    "\n",
    "plot_losses(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 5**: implement a dropout layer between the LSTM and the fully connected layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at Word Embeddings\n",
    "\n",
    "The word embeddings calculated during training can be extracted from the models and they can be used to calculate a semantic similarity between words.\n",
    "\n",
    "> **Task 6** : include the following code into the LSTM implementation\n",
    "\n",
    "```\n",
    "def get_embedding_for(self, w):\n",
    "    idx = vocab.lookup_indices([w])\n",
    "    return self.embedding(torch.Tensor(idx).int())\n",
    "````\n",
    "> and test the following block:\n",
    "\n",
    "you should see that words that are intuitively semantically closer have higher similarity scores (according to cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1=(lstm.get_embedding_for(\"great\").detach().numpy()).reshape(1,-1)\n",
    "v2=(lstm.get_embedding_for(\"bad\").detach().numpy()).reshape(1,-1)\n",
    "v3=(lstm.get_embedding_for(\"awful\").detach().numpy()).reshape(1,-1)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = dot(v1, v2.T)/(norm(v1)*norm(v2))\n",
    "\n",
    "print(\"cosine sim between great and bad\", cos_sim)\n",
    "\n",
    "cos_sim = dot(v2, v3.T)/(norm(v2)*norm(v3))\n",
    "\n",
    "print(\"cosine sim between bad and awful\", cos_sim)\n",
    "\n",
    "cos_sim = dot(v1, v3.T)/(norm(v1)*norm(v3))\n",
    "\n",
    "print(\"cosine sim between great and awful\", cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
