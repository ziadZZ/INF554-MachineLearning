{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import svd\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "\n",
    "## The perceptron\n",
    "\n",
    "We can think of the perceptron as the simplest neural network, composed by a single artificial neuron. An artificial neuron is a function of the input $\\mathbf{x}=(x_1, \\ldots , x_M)$ weighted by a vector of connection weights $\\mathbf{w}={w_1, \\ldots w_M}$, completed by a neuron bias $b$ and passed to an activation function $\\hat{y}=\\phi(f)$\n",
    "\n",
    "<img src=\"img/perceptron.png\" alt=\"Perceptron\" width=\"350\"/>\n",
    "\n",
    "$$ \\hat{y} = \\phi(\\mathbf{w}^\\top \\mathbf{x} + b )$$\n",
    "\n",
    "Learning with the perceptron consists in updating the weights:\n",
    "\n",
    "$$\\mathbf{w}^{(t+1)}= \\mathbf{w}^t + \\eta (y_i -\\hat{y_i})x_i $$ and\n",
    "$$b^{(t+1)}= b^t + \\eta (y_i -\\hat{y_i})$$\n",
    "\n",
    "with $i \\in {1, \\ldots, N}$, where $\\eta$ is the learning rate, $y_i$ is the correct output for the input $x_i$ and $N$ is the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "fname='data1.txt'\n",
    "data = np.loadtxt('data/%s' % fname, delimiter=',')\n",
    " \n",
    "X = data[:, 0:2] \n",
    "y = data[:, 2]\n",
    " \n",
    "# Plot data \n",
    "plt.plot(X[:,0][y == 1], X[:,1][y == 1], 'r+')\n",
    "plt.plot(X[:,0][y == 0], X[:,1][y == 0], 'bx')\n",
    "plt.title(fname)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 1**: Implement the perceptron algorithm, using the step activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    # Step activation\n",
    "    if x > 0 : return 1\n",
    "    else : return 0\n",
    "\n",
    "def perceptron(W,b,x,y,lr):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      W (np.array): (INPUT_SHAPE, 1) vector of weights\n",
    "      b : bias\n",
    "      x (np.array): (INPUT_SHAPE, 1) input vector\n",
    "      y : output label (1 or 0)\n",
    "      lr : learning rate\n",
    "    Returns:\n",
    "        (err, W, b) : err is 1 if the predicted label is different from y, 0 otherwise\n",
    "        W, b : the updated weights and bias\n",
    "    \"\"\"\n",
    "    #insert your solution here\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run the perceptron on the dataset above and display the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights and b\n",
    "b = 0.0\n",
    "W = rand.normal(size=(X.shape[1], 1))\n",
    "\n",
    "errors=1\n",
    "while errors > 0:\n",
    "    # start epoch\n",
    "    errors=0\n",
    "    for i in range(X.shape[0]):\n",
    "        xi = X[i,:].reshape(-1,1)\n",
    "        yi = y[i]\n",
    "        (err, W, b) = perceptron(W,b,xi,yi, 0.1)\n",
    "        errors+=err\n",
    "    \n",
    "#plot decision boundary\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[:,0][y == 1], X[:,1][y == 1], 'r+', label=\"$y=1$\")\n",
    "ax.plot(X[:,0][y == 0], X[:,1][y == 0], 'bx', label=\"$y=0$\")\n",
    "u = np.linspace(min(X[:, 0]),max(X[:, 0]), 200)\n",
    "v = np.linspace(min(X[:, 1]),max(X[:, 1]), 200)\n",
    "z = np.zeros(shape=(len(u), len(v)))\n",
    "for i in range(len(u)):\n",
    "    for j in range(len(v)):\n",
    "        x = np.array([[u[i],v[j]]])\n",
    "        z[i, j] = W[0]*u[i]+W[1]*v[j]+b\n",
    "CS=plt.contour(u,v,z.T)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"network\" for multi-class classification\n",
    "\n",
    "A perceptron will converge only if the classes are linearly separable. Another problem of the perceptron is that it can only be used for binary classification.\n",
    "\n",
    "In this section we are going to create a \"single-layer network\" where the inputs are directly connected to the output layer, in which we have one output unit for each class.\n",
    "\n",
    "Let's define a *linear layer* as:\n",
    "\n",
    "$$ \\mathbf{z} = W \\mathbf{x} + b $$\n",
    "\n",
    "With $W \\in \\mathbb{R}^{M \\times C} $. $M$ is the dimension of the input and $C$ the number of output units (= number of classes), $\\mathbf{x} \\in \\mathbb{R}^M$ and $b\\in \\mathbb{R}^C$.\n",
    "\n",
    "The Softmax function maps a vector $z \\in \\mathbb{R}^C$ to a vector $q \\in \\mathbb{R}^C$  such that:\n",
    "\n",
    "$$ q_i(\\mathbf{z}) = \\frac{e^{z_i}}{\\sum_{j \\in {\\{1, \\ldots, C\\}}}{e^{z_j}}} \\forall i \\in {\\{1, \\ldots, C\\}} = Softmax(\\mathbf{z})_i $$\n",
    "\n",
    "Note that: $ 0 \\leq q_i \\leq 1 \\forall i \\in {\\{1, \\ldots, C\\}} $, and: $\\sum_{i \\in {\\{1, \\ldots, C\\}}}{q_i} = 1 $; therefore we can interpret the Softmax function as a function that can normalize any real vector $\\mathbf{z}$  into a probability distribution $\\mathbf{q}$ over the $C$ values.\n",
    "\n",
    "With this setup, a \"forward pass\" corresponds to calculating $\\mathbf{z}$ and applying the Softmax to it.\n",
    "\n",
    "The implementation of the Softmax and a forward pass is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Takes the output of a layer as input and returns a probability distribution \n",
    "    input:\n",
    "        z (np.array)\n",
    "    \n",
    "    returns:\n",
    "        z (np.array)\n",
    "    \"\"\"\n",
    "    out = np.exp(z)\n",
    "    \n",
    "    return out/np.sum(out)\n",
    "\n",
    "def forward(W, b, x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass and returns the softmax\n",
    "    input:\n",
    "        W (np.array): (N_CLASSES, INPUT_SHAPE) The weight matrix of the perceptron\n",
    "        b (np.array): (N_CLASSES, 1) The bias matrix of the perceptron\n",
    "        x (np.array): (INPUT_SHAPE, 1) The input of the perceptron\n",
    "\n",
    "    returns:\n",
    "        (np.array) (C, 1)\n",
    "    \"\"\"\n",
    "    out = W @ x + b\n",
    "    \n",
    "    return softmax(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a supervised classification task, we typically use the cross-entropy function on top of the softmax output as a loss function. We use a 1-hot encoded vector for the true distribution $\\mathbf{p}(x)$ , where we have $1$ in the position corresponding to the true label $y$ and $0$ elsewhere:\n",
    "\n",
    "$$\n",
    "p_i(x) = \\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    1, & \\text{if}\\ y=i \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "and the output of the softmax function as our $\\mathbf{q}$.\n",
    "\n",
    "For a single sample $x$, the cross-entropy loss value for these p(x) and q(x) is then:\n",
    "$$H(p,q)= âˆ’ \\sum_{x}{p(x)\\log q(x)}$$\n",
    "\n",
    "Given that the only non-zero element of the 1-hot vector $p(x)$ is at the $y$ index, in practice the $p(x)$ vector is a selector for the $y$ index in the $q(x)$ vector. Therefore, the loss function for a single sample becomes:\n",
    "\n",
    "$$ Loss = -\\log(q_y) = - \\log \\left( \\frac{e^{z_y}}{\\sum_{j \\in {\\{1, \\ldots, C\\}}}{e^{z_j}}} \\right) = - z_y + \\log \\sum_{j}{e^{z_j}}$$\n",
    "\n",
    "> **Ques 1**: Derive the gradients of loss with respect to $W$ and $b$.\n",
    "\n",
    "$$\\nabla_W loss = ?$$\n",
    "$$\\nabla_b loss = ?$$\n",
    "\n",
    "> **Task 2**: Implement the gradients and the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads(softmaxed, x, y):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        softmaxed (np.array): (N_CLASSES, 1) result of the forward pass\n",
    "        y (np.array): (N_CLASSES, 1) One-hot encoded vector of the label\n",
    "        x (np.array): (INPUT_SHAPE, 1) Input vector\n",
    "        \n",
    "    returns:\n",
    "        d_W (np.array): (N_CLASSES, INPUT_SHAPE) Gradient of the loss with respect to the weight matrix\n",
    "        d_b (np.array): (N_CLASSES, 1) Gradient of the loss with respect to the bias matrix \n",
    "    \"\"\"\n",
    "    #insert your solution here\n",
    "    \n",
    "    \n",
    "    return d_W, d_b\n",
    "\n",
    "def compute_loss(softmaxed, y):\n",
    "    \"\"\"\n",
    "    loss for a single datapoint\n",
    "    inputs:\n",
    "        softmaxed (np.array): (N_CLASSES, 1)\n",
    "        y (np.array): (N_CLASSES, 1)\n",
    "        \n",
    "    returns:\n",
    "        (float)\n",
    "    \"\"\"\n",
    "    #insert your solution here\n",
    "    \n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_blobs(n_samples=500, n_features=2, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=Y)\n",
    "plt.show()\n",
    "\n",
    "#encode output as one-hot\n",
    "Y = OneHotEncoder(categories='auto').fit_transform(Y.reshape(-1,1)).toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42, test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function trains our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X_train, Y_train, X_test, Y_test, lr,\n",
    "                    n_it=5000, batch_size=10, random_seed=42):\n",
    "    \n",
    "    INPUT_SHAPE = X_train.shape[1]\n",
    "    N_CLASSES = Y_train.shape[1]\n",
    "    \n",
    "    # Initialise metrics lists\n",
    "    loss = []\n",
    "    batch_loss = []\n",
    "    acc = []\n",
    "    batch_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # Initialisation of the weigths\n",
    "    np.random.seed(random_seed)\n",
    "    b = rand.normal(size=(N_CLASSES, 1))\n",
    "    W = rand.normal(size=(N_CLASSES, INPUT_SHAPE))\n",
    "    \n",
    "    # Shuffling data for SGD\n",
    "    indexes = rand.randint(X_train.shape[0], size=n_it)\n",
    "    # training loop\n",
    "    for it, i in enumerate(indexes):\n",
    "        x = X_train[i,:].reshape(-1,1)\n",
    "        y = Y_train[i,:].reshape(-1,1)\n",
    "        \n",
    "        # Forward pass\n",
    "        softmaxed = forward(W, b, x)\n",
    "        \n",
    "        # Gradient calculation\n",
    "        d_W, d_b = compute_grads(softmaxed, x, y)\n",
    "        W -= lr * d_W\n",
    "        b -= lr * d_b\n",
    "        \n",
    "        # Metrics recording\n",
    "        batch_loss.append(compute_loss(softmaxed, y))\n",
    "        batch_acc.append(np.argmax(softmaxed) == np.argmax(y))\n",
    "        \n",
    "        # Test loop\n",
    "        if it % batch_size == 0:\n",
    "            acc_temp = []\n",
    "            for i in range(X_test.shape[0]):\n",
    "                x = X_test[i,:].reshape(-1,1)\n",
    "                y = Y_test[i,:].reshape(-1,1)\n",
    "                softmaxed = forward(W, b, x)\n",
    "                acc_temp.append(np.argmax(softmaxed) == np.argmax(y))\n",
    "                \n",
    "            test_acc.append(np.mean(acc_temp))\n",
    "            loss.append(np.mean(batch_loss))\n",
    "            acc.append(np.mean(batch_acc))\n",
    "            batch_loss=[]\n",
    "            batch_acc=[]\n",
    "\n",
    "    return W, b, loss, acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model on the data and observe the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, loss, acc, test_acc = train_network(X_train, Y_train, X_test, Y_test, lr=0.01, n_it=200)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8))\n",
    "ax1.plot(loss)\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax2.plot(test_acc, c='r', label='test')\n",
    "ax2.plot(acc, c='b', label='train')\n",
    "ax2.set_title(\"Train and test accuracy\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can visualize the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision(X, Y, forward_fun, figure=None):\n",
    "    \"\"\"Plots the decision function of a perceptron with respect to a forward funciton\n",
    "    \n",
    "    input:\n",
    "        X, Y (np.array): Test data\n",
    "        forward_fun (function): only accepts x as input (Ex: lambda x: forward_network(W, b, x))\n",
    "        figure (plt.figure): optional, usefull if you dont want to generate any new figure, \n",
    "                    in the case of suplots.\n",
    "    \"\"\"\n",
    "    markers=[\".\", \"*\", \"D\"]\n",
    "    low0, high0 = np.min(X[:,0]), np.max(X[:,0])\n",
    "    low1, high1 = np.min(X[:,1]), np.max(X[:,1])\n",
    "    data = np.zeros((100,100,Y.shape[1]))\n",
    "    for i1, x1 in enumerate(np.linspace(low0,high0,100)):\n",
    "        for i2, x2 in enumerate(np.linspace(low1,high1,100)):\n",
    "            x = np.array([x1, x2]).reshape(-1, 1)\n",
    "            softmaxed = forward_fun(x)\n",
    "            data[i2, i1, :] = softmaxed.reshape(-1)\n",
    "    if Y.shape[1] < 3:\n",
    "        data = data[:,:,0]\n",
    "    \n",
    "    if figure is None:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        \n",
    "    plt.imshow(data, extent=(low0,high0,low1,high1), origin='lower', interpolation='gaussian')\n",
    "    for c in range(Y.shape[1]):\n",
    "        plt.scatter(X[np.argmax(Y, 1) == c, 0], X[np.argmax(Y, 1) == c, 1], c='k', \n",
    "                    marker=markers[c], label=\"class %i\" % c)\n",
    "        \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(X_test, Y_test, lambda x: forward(W, b, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we used was linearly separable, so let's take a look at how our network would perform on a different dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_moons(n_samples=500, noise=.2)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:,0], X[:,1], c=Y)\n",
    "plt.show()\n",
    "Y = OneHotEncoder(categories='auto').fit_transform(Y.reshape(-1,1)).toarray()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, loss, acc, test_acc = train_network(X_train, Y_train, X_test, Y_test, 0.01, n_it=1000)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8))\n",
    "ax1.plot(loss)\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax2.plot(test_acc, c='r', label='test')\n",
    "ax2.plot(acc, c='b', label='train')\n",
    "ax2.set_title(\"Train and test accuracy\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plot_decision(X_test, Y_test, lambda x: forward(W,b,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first neural network\n",
    "\n",
    "Actually our \"network\" until now was nothing else than multinomial Logistic Regression. Therefore, to obtain a non-linear decision boundary, we need at least one **hidden layer** with a non-linear activation function; in this case we are going to study a case in which the activation chosen for the hidden layer is the $ReLU$ function:\n",
    "\n",
    "$$ z = W x + b_1 $$\n",
    "\n",
    "$$ h = ReLU(W x + b_1) = ReLU(z) $$\n",
    "\n",
    "We will use the Softmax seen in the previous case as the output layer:\n",
    "\n",
    "$$ \\theta = U h + b_2 $$\n",
    "\n",
    "$$ \\hat{y} = Softmax(\\theta) $$\n",
    "\n",
    "With $U \\in \\mathbb{R}^{N \\times H}$ and $W \\in \\mathbb{R}^{C \\times H}$ and $b_1$, $b_2$ vectors of corresponding dimensions. $H$ is the number of hidden units.\n",
    "\n",
    "The $ReLU$ function is defined as:\n",
    "\n",
    "$$ ReLU(x) = \n",
    "\\begin{cases}\n",
    "0 \\textrm{ if } x < 0\\\\\n",
    "x \\textrm{ otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and it's immediate to observe that the derivative of the ReLU function is: $sgn(ReLU(x)) $.\n",
    "\n",
    "> **Task 3**: Implement the ReLU activation function, the derivative of the relu, and the forward pass for neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x (np.array)\n",
    "    \n",
    "    returns:\n",
    "        x (np.array)\n",
    "    \"\"\"\n",
    "    #insert your solution here\n",
    "    \n",
    "    \n",
    "\n",
    "def d_relu(x):\n",
    "    \"\"\"Computes the derivative of the relu\n",
    "    input:\n",
    "        x (np.array)\n",
    "    \n",
    "    returns:\n",
    "        x (np.array)\n",
    "    \"\"\"\n",
    "    #insert your solution here\n",
    "    \n",
    "def forward_NN(U, b2, W, b1, x):\n",
    "    \"\"\"Forward pass of a two layer perceptron with relu activation\n",
    "    input:\n",
    "        W (np.array): (HIDDEN_SHAPE, INPUT_SHAPE) The weight matrix of the hidden layer\n",
    "        U (np.array): (N_CLASSES, HIDDEN_SHAPE) The weight matrix of the output layer\n",
    "        b1 (np.array): (HIDDEN_SHAPE, 1) The bias matrix of the hidden layer\n",
    "        b2 (np.array): (N_CLASSES, 1) The bias matrix of the output layer\n",
    "        x (np.array): (INPUT_SHAPE, 1) The input of the perceptron\n",
    "\n",
    "    returns:\n",
    "        softmaxed (np.array): (N_CLASSES, 1) the output of the network after final activation\n",
    "        hidden (np.array): (HIDDENT_SHAPE, 1) the output of the hidden layer after activation\n",
    "        out (np.array): (N_CLASSES, 1) the output of the network before final activation\n",
    "    \"\"\"\n",
    "    #insert your solution here\n",
    "    \n",
    "    return softmaxed, hidden, out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of the above together, we can see that:\n",
    "\n",
    "\n",
    "$$ q_i(\\mathbf{\\theta}) = \\frac{e^{\\theta_i}}{\\sum_{j \\in {\\{1, \\ldots, C\\}}}{e^{\\theta_j}}} \\forall i \\in {\\{1, \\ldots, C\\}} = Softmax(\\mathbf{\\theta})_i = \\hat{y}_i$$\n",
    "\n",
    "and therefore the Cross-Entropy loss is:\n",
    "\n",
    "$$\n",
    "Loss = -\\log(\\hat{y}_y) = - \\log \\left( \\frac{e^{\\theta_y}}{\\sum_{j \\in {\\{1, \\ldots, C\\}}}{e^{\\theta_j}}} \\right) = - \\theta_y + \\log \\sum_{j}{e^{\\theta_j}}$$\n",
    "\n",
    "> **Ques 2** : Derive the gradients of the loss with respect to $W$, $U$, $b_1$ and $b_2$.\n",
    "\n",
    "> **Task 4**: Based on the previous result implement the computation of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads_NN(hidden, softmaxed, U, x, y, W, b1):\n",
    "    \"\"\"Forward pass of a teo layer perceptron with relu activation\n",
    "    input:\n",
    "        hidden (np.array): (HIDDEN_SHAPE, 1) the output of the hidden layer after activation\n",
    "        softmaxed (np.array): (N_CLASSES, 1) the output of the network after final activation\n",
    "        U (np.array): (N_CLASSES, HIDDEN_SHAPE) The weight matrix of the output layer\n",
    "        x (np.array): (INPUT_SHAPE, 1) The input of the perceptron\n",
    "        y (np.array): (N_CLASSES, 1) Ground truth class\n",
    "    \n",
    "    returns:\n",
    "        d_U (np.array): (N_CLASSES, HIDDEN_SHAPE) Gradient with respect \n",
    "                        to the weight matrix of the output layer\n",
    "        d_b2 (np.array): (N_CLASSES, 1) Gradient with respect to the bias matrix of the output layer\n",
    "        d_W (np.array): (HIDDEN_SHAPE, INPUT_SHAPE) Gradient with respect to the \n",
    "                        weight matrix of the hidden layer\n",
    "        d_b1 (np.array): (HIDDEN_SHAPE, 1) Gradient with respect to the bias matrix of the hidden layer\n",
    "    \"\"\"\n",
    "    \n",
    "    #insert here your code\n",
    "    \n",
    "    \n",
    "    return d_U, d_b2, d_W, d_b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(X_train, Y_train, X_test, Y_test, lr, n_hidden,\n",
    "                    n_it=5000, batch_size=100, random_seed=42):\n",
    "\n",
    "    HIDDEN_SHAPE = n_hidden\n",
    "    INPUT_SHAPE = X_train.shape[1]\n",
    "    N_CLASSES = Y_train.shape[1]\n",
    "    \n",
    "    # Initialise metrics lists\n",
    "    loss = []\n",
    "    acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # Initialisation of the weigths\n",
    "    np.random.seed(random_seed)\n",
    "    b1 = rand.normal(size=(HIDDEN_SHAPE, 1))\n",
    "    W = rand.normal(size=(HIDDEN_SHAPE, INPUT_SHAPE))\n",
    "    b2 = rand.normal(size=(N_CLASSES, 1))\n",
    "    U = rand.normal(size=(N_CLASSES, HIDDEN_SHAPE))\n",
    "    \n",
    "    # Shuffling data\n",
    "    indexes = rand.randint(X_train.shape[0], size=n_it)\n",
    "    \n",
    "    # training loop\n",
    "    print(enumerate(indexes))\n",
    "    for it, i in enumerate(indexes):\n",
    "        \n",
    "        x = X_train[i,:].reshape(-1,1)\n",
    "        y = Y_train[i,:].reshape(-1,1)\n",
    "\n",
    "        # Forward pass\n",
    "        softmaxed, hidden, out = forward_NN(U, b2, W, b1, x)\n",
    "        # Back propagation\n",
    "        d_U, d_b2, d_W, d_b1 = compute_grads_NN(hidden, softmaxed, U, x, y, W, b1)\n",
    "        \n",
    "        #update weights\n",
    "        b1 -= lr * d_b1  \n",
    "        W -= lr * d_W\n",
    "        b2 -= lr * d_b2\n",
    "        U -= lr * d_U   \n",
    "        \n",
    "        # Metrics recording\n",
    "        loss.append(compute_loss(softmaxed, y))\n",
    "        acc.append(np.argmax(softmaxed) == np.argmax(y))\n",
    "        \n",
    "        # Test loop\n",
    "        if it % batch_size == 0:\n",
    "            acc_temp = []\n",
    "            for i in range(X_test.shape[0]):\n",
    "                x = X_test[i,:].reshape(-1,1)\n",
    "                y = Y_test[i,:].reshape(-1,1)\n",
    "                softmaxed, _, _ = forward_NN(U, b2, W, b1, x)\n",
    "                acc_temp.append(np.argmax(softmaxed) == np.argmax(y))\n",
    "                \n",
    "            test_acc.append(np.mean(acc_temp))\n",
    "\n",
    "    return U, b2, W, b1, loss, acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, b2, W, b1, loss, acc, test_acc = train_NN(X_train, Y_train, X_test, Y_test, 0.1, 16)\n",
    "\n",
    "def rollmean(x, window):\n",
    "    return [np.mean(x[i*window:(i+1)*window]) for i in range(len(x)//window)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8))\n",
    "window=100\n",
    "ax1.plot(rollmean(loss, window))\n",
    "ax1.set_title(\"Training Loss\")\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax2.plot(test_acc, c='r', label='test')\n",
    "ax2.plot(rollmean(acc, window), c='b', label='train')\n",
    "ax2.set_title(\"Train and test accuracy\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(X_test, Y_test, lambda x: forward_NN(U, b2, W, b1, x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify how the network performs if we modify the size of the hidden layer (2, 4, 8, 16 hidden units):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "for i, n_units in enumerate([2,4,8,16]):\n",
    "    U, b2, W, b1, loss, acc, test_acc = train_NN(X_train, Y_train, X_test, Y_test, \n",
    "                                                          0.1, n_units, n_it=5000)\n",
    "    plot_decision(X_test, Y_test, lambda x: forward_NN(U, b2, W, b1, x)[0],\n",
    "                  plt.subplot(2,2,i+1, title=\"%i units\" % n_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like now to study the effect of using a different activation function. We will change ReLU to the hyperbolic tangent activation (or *tanh*):\n",
    "\n",
    "$$\n",
    "tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1}\n",
    "$$\n",
    "\n",
    "(Note: you may use the Numpy implementation of tanh)\n",
    "\n",
    "with derivative (necessary to update the gradient computation): $ tanh'(x) =  1 - tanh(x)^2$ \n",
    "\n",
    "\n",
    "> **Task 5**: Update the forward pass and gradient computation of our neural network implementation to use the tanh activation function and vizualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_NN(U, b2, W, b1, x):\n",
    "    \n",
    "    #insert your code here\n",
    "    \n",
    "    return softmaxed, hidden, out\n",
    "    \n",
    "def compute_grads_NN(hidden, softmaxed, U, x, y, W, b1):\n",
    "    \n",
    "    #insert here your code\n",
    "    \n",
    "    \n",
    "    return d_U, d_b2, d_W, d_b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "for i, n_units in enumerate([2, 4, 8, 16]):\n",
    "    U, b2, W, b1, loss, acc, test_acc = train_NN(X_train, Y_train, X_test, Y_test, \n",
    "                                                          0.1, n_units, n_it=1000)\n",
    "    plot_decision(X_test, Y_test, lambda x: forward_NN(U, b2, W, b1, x)[0],\n",
    "                  plt.subplot(2,2,i+1, title=\"%i units\" % n_units))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
